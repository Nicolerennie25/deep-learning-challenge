# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HcJ2NqLPrd3KQ0hM_ZjutPMphylLxHJX
"""

!pip install tensorflow

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Load data
for filename in uploaded.keys():
    print(f'User uploaded file "{filename}" with length {len(uploaded[filename])} bytes')

# Load the dataset using the uploaded filename
df = pd.read_csv(filename)

# Display the first few rows of the dataframe
df.head()

# Display the column names
print(df.columns)

# Drop the non-beneficial ID columns (replace with actual column names)
df = df.drop(columns=['EIN', 'NAME'])  # Example column names

# Display the first few rows after dropping the columns to verify
df.head()

# Check unique values for each column
print(df.nunique())

# Combine rare categorical variables (update column name as needed)
application_counts = df['APPLICATION_TYPE'].value_counts()  # Replace 'APPLICATION_TYPE' with the actual column name if different
rare_applications = application_counts[application_counts < 100].index
df['APPLICATION_TYPE'] = df['APPLICATION_TYPE'].apply(lambda x: 'Other' if x in rare_applications else x)

# Encode categorical variables
df = pd.get_dummies(df)

# Split into features and target arrays
X = df.drop(columns=['IS_SUCCESSFUL'])  # Ensure 'IS_SUCCESSFUL' is the target column
y = df['IS_SUCCESSFUL']

# Split the data into training and testing datasets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the model
model = Sequential()

# Add first hidden layer
model.add(Dense(80, input_dim=X_train_scaled.shape[1], activation='relu'))

# Add second hidden layer
model.add(Dense(30, activation='relu'))

# Add output layer
model.add(Dense(1, activation='sigmoid'))

# Print model summary
model.summary()

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_scaled, y_train, epochs=100, callbacks=[tf.keras.callbacks.ModelCheckpoint('AlphabetSoupCharity.h5', save_weights_only=True, save_freq=5)])

# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Loss: {loss}, Accuracy: {accuracy}")

# Save the model
model.save('AlphabetSoupCharity.h5')

# Save the optimized model to a file in the recommended Keras format
model.save('AlphabetSoupCharity_Optimization.keras')

from google.colab import files

# Download the model file
files.download('AlphabetSoupCharity_Optimization.keras')